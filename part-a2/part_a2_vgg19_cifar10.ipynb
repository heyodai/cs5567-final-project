{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMpLvEROy4f"
      },
      "source": [
        "**Configuration**\n",
        "- Model: `VGG19`\n",
        "- Dataset: `CIFAR10`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU torch torchvision tdqm accelerate"
      ],
      "metadata": {
        "id": "BvSKeHiHO0ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fce3485-7fd7-4a90-8cf2-a84dd13416e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nVTwA6qVOy4j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nSr-O761Oy4k"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARN_RATE = 0.001\n",
        "NUM_EPOCHS = 5 # Much more than this isn't feasible w/o better hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQZbkQaBOy4k"
      },
      "outputs": [],
      "source": [
        "# TensorBoard setup\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = SummaryWriter(f\"runs/part_a2_{current_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54F8Oe0Oy4l"
      },
      "source": [
        "### Step 1. Load and Transform Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rc5RBEqAOy4l"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 224x224 for VGG19\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvsvwugeOy4l",
        "outputId": "2f92e884-d3af-407a-da6a-7b1593b2c0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 41247521.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYCqT2-Oy4m",
        "outputId": "35cce6fd-41c6-4c20-f0ad-f8ea3126ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noA4xTn0Oy4n"
      },
      "source": [
        "### Step 2. Setup the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ro-8ZyOy4n",
        "outputId": "15ad1e09-037f-4b1a-ffc3-954628ef306d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 205MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "vgg19 = models.vgg19(pretrained=True)\n",
        "vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SyGSbKwFOy4n"
      },
      "outputs": [],
      "source": [
        "# Freeze the features part\n",
        "for param in vgg19.features.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2xjTQ1BOy4n"
      },
      "outputs": [],
      "source": [
        "# Modify the classifier part\n",
        "vgg19.classifier[6] = nn.Linear(4096, 10)  # CIFAR10 has 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65PObCi0Oy4n",
        "outputId": "f50f8b77-6f55-47e9-91bd-f4089e753bd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Make sure we're using GPU\n",
        "device = torch.device(\n",
        "    \"mps\"  # for macOS\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "vgg19 = vgg19.to(device)\n",
        "\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEuL9HNOy4o"
      },
      "source": [
        "### Step 3. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zerieDEEOy4o"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(vgg19.classifier.parameters(), lr=LEARN_RATE)  # Only train the classifier parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83rW7_9Oy4o",
        "outputId": "44ae379d-5691-4f6a-ef37-f3d7eb8a7b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 100 loss 1.2046242707967758 accuracy 0.62\n",
            "Batch 200 loss 0.8886853566765786 accuracy 0.73625\n",
            "Batch 300 loss 0.9058354216814041 accuracy 0.7434375\n",
            "Batch 400 loss 0.8879805862903595 accuracy 0.750625\n",
            "Batch 500 loss 0.9046478420495987 accuracy 0.7421875\n",
            "Batch 600 loss 0.8912637141346932 accuracy 0.7503125\n",
            "Batch 700 loss 0.8647144854068756 accuracy 0.7559375\n",
            "Batch 800 loss 0.8268692925572395 accuracy 0.769375\n",
            "Batch 900 loss 0.8593142886459827 accuracy 0.76\n",
            "Batch 1000 loss 0.803871750831604 accuracy 0.7753125\n",
            "Batch 1100 loss 0.9012834990024566 accuracy 0.7525\n",
            "Batch 1200 loss 0.8840978801250458 accuracy 0.756875\n",
            "Batch 1300 loss 0.7939781923592091 accuracy 0.768125\n",
            "Batch 1400 loss 0.8513372954726219 accuracy 0.770625\n",
            "Batch 1500 loss 0.8073881351947785 accuracy 0.765\n",
            "Epoch 2/5\n",
            "Batch 100 loss 0.6544343698024749 accuracy 0.8171875\n",
            "Batch 200 loss 0.6739489133656025 accuracy 0.815\n",
            "Batch 300 loss 0.6567032957077026 accuracy 0.8178125\n",
            "Batch 400 loss 0.6635352427512408 accuracy 0.8059375\n",
            "Batch 500 loss 0.7237490311264991 accuracy 0.79625\n",
            "Batch 600 loss 0.6917626486718654 accuracy 0.8125\n",
            "Batch 700 loss 0.7223853282630444 accuracy 0.8021875\n",
            "Batch 800 loss 0.6757249237596988 accuracy 0.819375\n",
            "Batch 900 loss 0.6060844748467207 accuracy 0.823125\n",
            "Batch 1000 loss 0.6213556794822216 accuracy 0.8259375\n",
            "Batch 1100 loss 0.6605486454069615 accuracy 0.8203125\n",
            "Batch 1200 loss 0.6896811529994011 accuracy 0.8128125\n",
            "Batch 1300 loss 0.6855173851549625 accuracy 0.8128125\n",
            "Batch 1400 loss 0.7101413634419441 accuracy 0.811875\n",
            "Batch 1500 loss 0.7209684930741787 accuracy 0.8096875\n",
            "Epoch 3/5\n",
            "Batch 100 loss 0.5340915931761265 accuracy 0.846875\n",
            "Batch 200 loss 0.5467600850015879 accuracy 0.84375\n",
            "Batch 300 loss 0.6075114473700524 accuracy 0.841875\n",
            "Batch 400 loss 0.5746563431248068 accuracy 0.8453125\n",
            "Batch 500 loss 0.5869419665634632 accuracy 0.8396875\n",
            "Batch 600 loss 0.5817383046448231 accuracy 0.8446875\n",
            "Batch 700 loss 0.5142747674882412 accuracy 0.8575\n",
            "Batch 800 loss 0.5510619855299592 accuracy 0.8534375\n",
            "Batch 900 loss 0.5969916503131389 accuracy 0.834375\n",
            "Batch 1000 loss 0.6124697355180979 accuracy 0.834375\n",
            "Batch 1100 loss 0.5645647707954049 accuracy 0.8434375\n",
            "Batch 1200 loss 0.5712414975464344 accuracy 0.8425\n",
            "Batch 1300 loss 0.610071883648634 accuracy 0.8421875\n",
            "Batch 1400 loss 0.5648770072311163 accuracy 0.8428125\n",
            "Batch 1500 loss 0.6284073576331138 accuracy 0.8275\n",
            "Epoch 4/5\n",
            "Batch 100 loss 0.4797282082750462 accuracy 0.8640625\n",
            "Batch 200 loss 0.5390536160767079 accuracy 0.861875\n",
            "Batch 300 loss 0.4413373076170683 accuracy 0.86625\n",
            "Batch 400 loss 0.49665032275021076 accuracy 0.8715625\n",
            "Batch 500 loss 0.505493705123663 accuracy 0.865625\n",
            "Batch 600 loss 0.5153810342028737 accuracy 0.8625\n",
            "Batch 700 loss 0.4989878172427416 accuracy 0.8603125\n",
            "Batch 800 loss 0.537890110462904 accuracy 0.865\n",
            "Batch 900 loss 0.5483502642810345 accuracy 0.849375\n",
            "Batch 1000 loss 0.5885596871376038 accuracy 0.8440625\n",
            "Batch 1100 loss 0.4969056445360184 accuracy 0.8559375\n",
            "Batch 1200 loss 0.44216861329972745 accuracy 0.876875\n",
            "Batch 1300 loss 0.5718537736311555 accuracy 0.85625\n",
            "Batch 1400 loss 0.5573987871780992 accuracy 0.8534375\n",
            "Batch 1500 loss 0.5718815445899963 accuracy 0.8509375\n",
            "Epoch 5/5\n",
            "Batch 100 loss 0.45507091291248797 accuracy 0.8696875\n",
            "Batch 200 loss 0.44754013273864984 accuracy 0.8728125\n",
            "Batch 300 loss 0.445609371997416 accuracy 0.8775\n",
            "Batch 400 loss 0.4491100124269724 accuracy 0.8815625\n",
            "Batch 500 loss 0.43972126111388204 accuracy 0.8784375\n",
            "Batch 600 loss 0.49519189655780793 accuracy 0.8721875\n",
            "Batch 700 loss 0.4218562350980937 accuracy 0.8790625\n",
            "Batch 800 loss 0.4874722381308675 accuracy 0.8803125\n",
            "Batch 900 loss 0.4852748558297753 accuracy 0.8628125\n",
            "Batch 1000 loss 0.5347448436915875 accuracy 0.860625\n",
            "Batch 1100 loss 0.5167685686796903 accuracy 0.864375\n",
            "Batch 1200 loss 0.4770237079635262 accuracy 0.8725\n",
            "Batch 1300 loss 0.49981858987361194 accuracy 0.86125\n",
            "Batch 1400 loss 0.5179447356611491 accuracy 0.8621875\n",
            "Batch 1500 loss 0.5238864328712225 accuracy 0.8678125\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, criterion, optimizer, trainloader, num_epochs, device, writer):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print(f'Batch {i + 1} loss {running_loss / 100} accuracy {correct / total}')\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "train_model(vgg19, criterion, optimizer, trainloader, NUM_EPOCHS, device, writer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, criterion, testloader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the test images: {accuracy} %')\n",
        "    return test_loss, accuracy\n",
        "\n",
        "evaluate_model(vgg19, criterion, testloader, device)"
      ],
      "metadata": {
        "id": "ePdNYArPIJHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba75b65f-1e29-427f-8169-224857c69dbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 86.14 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5278311101154397, 86.14)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(vgg19.state_dict(), 'part_a2_vgg19_cifar10_model.pth')"
      ],
      "metadata": {
        "id": "knBMRCNEpWge"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
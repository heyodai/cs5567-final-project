{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMpLvEROy4f"
      },
      "source": [
        "**Configuration**\n",
        "- Model: `ResNet50`\n",
        "- Dataset: `CIFAR10`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU torch torchvision tdqm accelerate"
      ],
      "metadata": {
        "id": "BvSKeHiHO0ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5c4d08-8199-467d-c6e6-eb66dcc58149"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nVTwA6qVOy4j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nSr-O761Oy4k"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARN_RATE = 0.001\n",
        "NUM_EPOCHS = 5 # Much more than this isn't feasible w/o better hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQZbkQaBOy4k"
      },
      "outputs": [],
      "source": [
        "# TensorBoard setup\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = SummaryWriter(f\"runs/part_a2_{current_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54F8Oe0Oy4l"
      },
      "source": [
        "### Step 1. Load and Transform Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rc5RBEqAOy4l"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 224x224 for ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvsvwugeOy4l",
        "outputId": "93324d3c-350f-454f-fe93-2c049ad48fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 76179289.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYCqT2-Oy4m",
        "outputId": "c3e43538-a742-4c37-de21-511877eacb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noA4xTn0Oy4n"
      },
      "source": [
        "### Step 2. Setup the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ro-8ZyOy4n",
        "outputId": "1ff3940c-a344-4027-d814-db9a10fdd33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 78.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SyGSbKwFOy4n"
      },
      "outputs": [],
      "source": [
        "# Freeze the features part\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2xjTQ1BOy4n"
      },
      "outputs": [],
      "source": [
        "# Modify the classifier part\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, 10)  # CIFAR10 has 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65PObCi0Oy4n",
        "outputId": "8027d986-85a3-46e5-c388-45112ed8a708"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Make sure we're using GPU\n",
        "device = torch.device(\n",
        "    \"mps\"  # for macOS\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "resnet50 = resnet50.to(device)\n",
        "\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEuL9HNOy4o"
      },
      "source": [
        "### Step 3. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zerieDEEOy4o"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(resnet50.fc.parameters(), lr=LEARN_RATE)  # Only train the fully connected layer parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83rW7_9Oy4o",
        "outputId": "e32767dd-4b7f-4a3d-8130-db8d2b131afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 100 loss 0.5856425519287586 accuracy 0.800625\n",
            "Batch 200 loss 0.5850911162793636 accuracy 0.79375\n",
            "Batch 300 loss 0.5999206832051277 accuracy 0.799375\n",
            "Batch 400 loss 0.5512286047637462 accuracy 0.8115625\n",
            "Batch 500 loss 0.5933993965387344 accuracy 0.793125\n",
            "Batch 600 loss 0.5865777930617333 accuracy 0.793125\n",
            "Batch 700 loss 0.6468075993657112 accuracy 0.7790625\n",
            "Batch 800 loss 0.6181457969546318 accuracy 0.7871875\n",
            "Batch 900 loss 0.5960128384828568 accuracy 0.7975\n",
            "Batch 1000 loss 0.598408582508564 accuracy 0.7965625\n",
            "Batch 1100 loss 0.5997065730392933 accuracy 0.7946875\n",
            "Batch 1200 loss 0.610958356410265 accuracy 0.7890625\n",
            "Batch 1300 loss 0.5853529576957226 accuracy 0.7965625\n",
            "Batch 1400 loss 0.6047178196907044 accuracy 0.793125\n",
            "Batch 1500 loss 0.6140363073348999 accuracy 0.79375\n",
            "Epoch 2/5\n",
            "Batch 100 loss 0.6049790528416633 accuracy 0.7928125\n",
            "Batch 200 loss 0.6083382558822632 accuracy 0.799375\n",
            "Batch 300 loss 0.5603594698011876 accuracy 0.8078125\n",
            "Batch 400 loss 0.5716924813389778 accuracy 0.79625\n",
            "Batch 500 loss 0.5846302691102028 accuracy 0.8009375\n",
            "Batch 600 loss 0.576920979321003 accuracy 0.798125\n",
            "Batch 700 loss 0.5946126502752304 accuracy 0.7940625\n",
            "Batch 800 loss 0.5605741553008556 accuracy 0.8121875\n",
            "Batch 900 loss 0.5191620568931102 accuracy 0.818125\n",
            "Batch 1000 loss 0.572387974113226 accuracy 0.804375\n",
            "Batch 1100 loss 0.5653105607628822 accuracy 0.8096875\n",
            "Batch 1200 loss 0.578633750975132 accuracy 0.7965625\n",
            "Batch 1300 loss 0.5836890372633934 accuracy 0.805625\n",
            "Batch 1400 loss 0.6062479904294014 accuracy 0.78625\n",
            "Batch 1500 loss 0.5822261810302735 accuracy 0.8\n",
            "Epoch 3/5\n",
            "Batch 100 loss 0.530889165699482 accuracy 0.8140625\n",
            "Batch 200 loss 0.5541786800324917 accuracy 0.8096875\n",
            "Batch 300 loss 0.5997363068163395 accuracy 0.7871875\n",
            "Batch 400 loss 0.5488309046626091 accuracy 0.805625\n",
            "Batch 500 loss 0.5662120340764523 accuracy 0.8046875\n",
            "Batch 600 loss 0.5464805218577385 accuracy 0.8109375\n",
            "Batch 700 loss 0.5430397922545671 accuracy 0.81375\n",
            "Batch 800 loss 0.5264894936978817 accuracy 0.8209375\n",
            "Batch 900 loss 0.599527857452631 accuracy 0.788125\n",
            "Batch 1000 loss 0.5188579508662223 accuracy 0.81625\n",
            "Batch 1100 loss 0.5648552457243204 accuracy 0.8090625\n",
            "Batch 1200 loss 0.561892063766718 accuracy 0.813125\n",
            "Batch 1300 loss 0.5508190341293812 accuracy 0.81\n",
            "Batch 1400 loss 0.5480226697027684 accuracy 0.8059375\n",
            "Batch 1500 loss 0.5747162245213986 accuracy 0.8\n",
            "Epoch 4/5\n",
            "Batch 100 loss 0.5338152735680342 accuracy 0.816875\n",
            "Batch 200 loss 0.5571345834434033 accuracy 0.80375\n",
            "Batch 300 loss 0.5477552777528762 accuracy 0.8059375\n",
            "Batch 400 loss 0.5583645285665989 accuracy 0.8096875\n",
            "Batch 500 loss 0.5557487899065018 accuracy 0.8146875\n",
            "Batch 600 loss 0.545421511977911 accuracy 0.8075\n",
            "Batch 700 loss 0.5168933262676001 accuracy 0.821875\n",
            "Batch 800 loss 0.542850112169981 accuracy 0.8075\n",
            "Batch 900 loss 0.5769231505692005 accuracy 0.80125\n",
            "Batch 1000 loss 0.5834410603344441 accuracy 0.8028125\n",
            "Batch 1100 loss 0.559231072217226 accuracy 0.8028125\n",
            "Batch 1200 loss 0.5478176155686378 accuracy 0.811875\n",
            "Batch 1300 loss 0.5404411290585994 accuracy 0.809375\n",
            "Batch 1400 loss 0.5214683114737272 accuracy 0.81625\n",
            "Batch 1500 loss 0.5969615225493908 accuracy 0.799375\n",
            "Epoch 5/5\n",
            "Batch 100 loss 0.5606635102629661 accuracy 0.8071875\n",
            "Batch 200 loss 0.5620235309004784 accuracy 0.8109375\n",
            "Batch 300 loss 0.5445134289562702 accuracy 0.82\n",
            "Batch 400 loss 0.5366041879355907 accuracy 0.81625\n",
            "Batch 500 loss 0.5817750249803066 accuracy 0.8034375\n",
            "Batch 600 loss 0.5487636154890061 accuracy 0.808125\n",
            "Batch 700 loss 0.5505850295722484 accuracy 0.81125\n",
            "Batch 800 loss 0.5292634384334087 accuracy 0.8090625\n",
            "Batch 900 loss 0.5529780194163323 accuracy 0.80625\n",
            "Batch 1000 loss 0.5457992312312127 accuracy 0.8115625\n",
            "Batch 1100 loss 0.5330625316500663 accuracy 0.8159375\n",
            "Batch 1200 loss 0.5083190038800239 accuracy 0.823125\n",
            "Batch 1300 loss 0.5591974139213562 accuracy 0.8046875\n",
            "Batch 1400 loss 0.5097681783139706 accuracy 0.824375\n",
            "Batch 1500 loss 0.5294599658250809 accuracy 0.825625\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, criterion, optimizer, trainloader, num_epochs, device, writer):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print(f'Batch {i + 1} loss {running_loss / 100} accuracy {correct / total}')\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "train_model(resnet50, criterion, optimizer, trainloader, NUM_EPOCHS, device, writer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, criterion, testloader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the test images: {accuracy} %')\n",
        "    return test_loss, accuracy\n",
        "\n",
        "evaluate_model(resnet50, criterion, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYk6LnS64Bmk",
        "outputId": "2e790614-001f-4544-a142-43e262ba0d70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 8.02 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.3740478605508044, 8.02)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(resnet50.state_dict(), 'part_a2_resnet_cifar10_model.pth')"
      ],
      "metadata": {
        "id": "knBMRCNEpWge"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
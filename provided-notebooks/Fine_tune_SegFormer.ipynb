{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5Gn7TiyIGv"
      },
      "source": [
        "## Fine-tune SegFormer on a custom dataset\n",
        "\n",
        "In this notebook, we are going to fine-tune [SegFormerForSemanticSegmentation](https://huggingface.co/docs/transformers/main/model_doc/segformer#transformers.SegformerForSemanticSegmentation) on a custom **semantic segmentation** dataset. In semantic segmentation, the goal for the model is to label each pixel of an image with one of a list of predefined classes.\n",
        "\n",
        "We load the encoder of the model with weights pre-trained on ImageNet-1k, and fine-tune it together with the decoder head, which starts with randomly initialized weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMYYJ7_do08a"
      },
      "source": [
        "!pip install -q transformers datasets evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxayjYMSEw9r"
      },
      "source": [
        "## Download toy dataset\n",
        "\n",
        "Here we download a small subset of the ADE20k dataset, which is an important benchmark for semantic segmentation. It contains 150 labels.\n",
        "\n",
        "I've made a small subset just for demonstration purposes (namely the 10 first training and 10 first validation images + segmentation maps). The goal for the model is to overfit this tiny dataset (because that makes sure that it'll work on a larger scale)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70LYEcMEwzk"
      },
      "source": [
        "import requests, zipfile, io\n",
        "\n",
        "def download_data():\n",
        "    url = \"https://www.dropbox.com/s/l1e45oht447053f/ADE20k_toy_dataset.zip?dl=1\"\n",
        "    r = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall()\n",
        "\n",
        "download_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this dataset is now also available on the hub :) you can directly check out the images [in your browser](scene_parse_150)! It can be easily loaded as follows (note that loading will take some time as the dataset is several GB's large):"
      ],
      "metadata": {
        "id": "TqLjSZ7lcpex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "load_entire_dataset = False\n",
        "\n",
        "if load_entire_dataset:\n",
        "  dataset = load_dataset(\"scene_parse_150\")"
      ],
      "metadata": {
        "id": "NmTmaA-ycttO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEXNfwpWsl"
      },
      "source": [
        "## Define PyTorch dataset and dataloaders\n",
        "\n",
        "Here we define a [custom PyTorch dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). Each item of the dataset consists of an image and a corresponding segmentation map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjHHo_eLpYMa"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class SemanticSegmentationDataset(Dataset):\n",
        "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, image_processor, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
        "            image_processor (SegFormerImageProcessor): image processor to prepare images + segmentation maps.\n",
        "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.image_processor = image_processor\n",
        "        self.train = train\n",
        "\n",
        "        sub_path = \"training\" if self.train else \"validation\"\n",
        "        self.img_dir = os.path.join(self.root_dir, \"images\", sub_path)\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"annotations\", sub_path)\n",
        "\n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "          image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "\n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "          annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
        "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "\n",
        "        # randomly crop + pad both image and segmentation map to same size\n",
        "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz8K8aFAzg0S"
      },
      "source": [
        "Let's initialize the training + validation datasets. Important: we initialize the image processor with `reduce_labels=True`, as the classes in ADE20k go from 0 to 150, with 0 meaning \"background\". However, we want the labels to go from 0 to 149, and only train the model to recognize the 150 classes (which don't include \"background\"). Hence, we'll reduce all labels by 1 and replace 0 by 255, which is the `ignore_index` of SegFormer's loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8GpVF2Dpkvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f542c8-6bf1-45c0-dbe4-e7aac01fd4a0"
      },
      "source": [
        "from transformers import SegformerImageProcessor\n",
        "\n",
        "root_dir = '/content/ADE20k_toy_dataset'\n",
        "image_processor = SegformerImageProcessor(reduce_labels=True)\n",
        "\n",
        "train_dataset = SemanticSegmentationDataset(root_dir=root_dir, image_processor=image_processor)\n",
        "valid_dataset = SemanticSegmentationDataset(root_dir=root_dir, image_processor=image_processor, train=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/segformer/image_processing_segformer.py:102: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7NqpQETFcoS"
      },
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(valid_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5H4Q1OK282K"
      },
      "source": [
        "Let's verify a random example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHPcEAEz13L8"
      },
      "source": [
        "encoded_inputs = train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNHZO5r22WJX"
      },
      "source": [
        "encoded_inputs[\"pixel_values\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CiJsa0z2Xdm"
      },
      "source": [
        "encoded_inputs[\"labels\"].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zPQ7_t_HbS9"
      },
      "source": [
        "encoded_inputs[\"labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4tOQeRfGKFt"
      },
      "source": [
        "encoded_inputs[\"labels\"].squeeze().unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfr3l91nrS-C"
      },
      "source": [
        "Next, we define corresponding dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4cU8hU3rZF-"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpjB47KB6iy3"
      },
      "source": [
        "batch = next(iter(train_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SSu3TwV6kXJ"
      },
      "source": [
        "for k,v in batch.items():\n",
        "  print(k, v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ki_uXjejQ23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811e9ceb-8ce0-474f-ca5f-cac6ed86a101"
      },
      "source": [
        "batch[\"labels\"].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXO_-Dtg2a6R"
      },
      "source": [
        "mask = (batch[\"labels\"] != 255)\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Memrbh1Z2eQL"
      },
      "source": [
        "batch[\"labels\"][mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R9lxUSq4mgr"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Here we load the model, and equip the encoder with weights pre-trained on ImageNet-1k (we take the smallest variant, `nvidia/mit-b0` here, but you can take a bigger one like `nvidia/mit-b5` from the [hub](https://huggingface.co/models?other=segformer)). We also set the `id2label` and `label2id` mappings, which will be useful when performing inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81dWx1WBqIos"
      },
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# load id2label mapping from a JSON on the hub\n",
        "repo_id = \"huggingface/label-files\"\n",
        "filename = \"ade20k-id2label.json\"\n",
        "id2label = json.load(open(hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
        "id2label = {int(k): v for k, v in id2label.items()}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# define model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
        "                                                         num_labels=150,\n",
        "                                                         id2label=id2label,\n",
        "                                                         label2id=label2id,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgyTbnRxXvF"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Here we fine-tune the model in native PyTorch, using the AdamW optimizer. We use the same learning rate as the one reported in the [paper](https://arxiv.org/abs/2105.15203).\n",
        "\n",
        "It's also very useful to track metrics during training. For semantic segmentation, typical metrics include the mean intersection-over-union (mIoU) and pixel-wise accuracy. These are available in the Datasets library. We can load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"mean_iou\")"
      ],
      "metadata": {
        "id": "iOdgd24YOSsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor.do_reduce_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8fJQRpnt-3A",
        "outputId": "9f739e1c-2750-4a76-e49b-de1a52618030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHdp6-w0wDei"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "   print(\"Epoch:\", epoch)\n",
        "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate\n",
        "        with torch.no_grad():\n",
        "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "          # note that the metric expects predictions + labels as numpy arrays\n",
        "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # let's print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "          # currently using _compute instead of compute\n",
        "          # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
        "          metrics = metric._compute(\n",
        "                  predictions=predicted.cpu(),\n",
        "                  references=labels.cpu(),\n",
        "                  num_labels=len(id2label),\n",
        "                  ignore_index=255,\n",
        "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
        "              )\n",
        "\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqG3SNZp9SxO"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Finally, let's check whether the model has really learned something.\n",
        "\n",
        "Let's test the trained model on an image (refer to my [inference notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Segformer_inference_notebook.ipynb) for details):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUh8rl_rWnwW"
      },
      "source": [
        "image = Image.open('/content/ADE20k_toy_dataset/images/training/ADE_train_00000002.jpg')\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmBTll7G-Pr7"
      },
      "source": [
        "# prepare the image for the model\n",
        "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "print(pixel_values.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ryo_eQ-dE8"
      },
      "source": [
        "import torch\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(pixel_values=pixel_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW_OjYNh-irt",
        "outputId": "fa43bb21-27bc-449a-9bc1-2d29b0d8fae1"
      },
      "source": [
        "# logits are of shape (batch_size, num_labels, height/4, width/4)\n",
        "logits = outputs.logits.cpu()\n",
        "print(logits.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 150, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnotB_Rd-tAF"
      },
      "source": [
        "def ade_palette():\n",
        "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
        "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
        "            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n",
        "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
        "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n",
        "            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n",
        "            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n",
        "            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n",
        "            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n",
        "            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n",
        "            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n",
        "            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n",
        "            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n",
        "            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n",
        "            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n",
        "            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n",
        "            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n",
        "            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n",
        "            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n",
        "            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n",
        "            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n",
        "            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n",
        "            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n",
        "            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n",
        "            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n",
        "            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n",
        "            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n",
        "            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n",
        "            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n",
        "            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n",
        "            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n",
        "            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n",
        "            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n",
        "            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n",
        "            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n",
        "            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n",
        "            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n",
        "            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n",
        "            [102, 255, 0], [92, 0, 255]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
        "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()\n",
        "print(predicted_segmentation_map)"
      ],
      "metadata": {
        "id": "JJDsDG_u2oke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH9c-TbA-xBV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
        "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "\n",
        "palette = np.array(ade_palette())\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[predicted_segmentation_map == label, :] = color\n",
        "# Convert to BGR\n",
        "color_seg = color_seg[..., ::-1]\n",
        "\n",
        "# Show image + mask\n",
        "img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "img = img.astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoPeZTsx_MoN"
      },
      "source": [
        "Compare this to the ground truth segmentation map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hsBaYTY-yuH"
      },
      "source": [
        "map = Image.open('/content/ADE20k_toy_dataset/annotations/training/ADE_train_00000002.png')\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbNeV9xdw7rm"
      },
      "source": [
        "# convert map to NumPy array\n",
        "import cv2\n",
        "map = np.array(map)\n",
        "map[map == 0] = 255 # background class is replaced by ignore_index\n",
        "map = map - 1 # other classes are reduced by one\n",
        "map[map == 254] = 255\n",
        "\n",
        "classes_map = np.unique(map).tolist()\n",
        "unique_classes = [model.config.id2label[idx] if idx!=255 else None for idx in classes_map]\n",
        "print(\"Classes in this image:\", unique_classes)\n",
        "\n",
        "# create coloured map\n",
        "color_seg = np.zeros((map.shape[0], map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "palette = np.array(ade_palette())\n",
        "for label, color in enumerate(palette):\n",
        "    color_seg[map == label, :] = color\n",
        "# Convert to BGR\n",
        "color_seg = cv2.resize(color_seg, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# Show image + mask\n",
        "img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "img = img.astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the metrics:"
      ],
      "metadata": {
        "id": "YYwTR246pgwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQksqdF5z7q0"
      },
      "source": [
        "# metric expects a list of numpy arrays for both predictions and references\n",
        "metrics = metric._compute(\n",
        "                  predictions=[predicted_segmentation_map],\n",
        "                  references=[map],\n",
        "                  num_labels=len(id2label),\n",
        "                  ignore_index=255,\n",
        "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.keys()"
      ],
      "metadata": {
        "id": "GZTl6XuEuK_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# print overall metrics\n",
        "for key in list(metrics.keys())[:3]:\n",
        "  print(key, metrics[key])\n",
        "\n",
        "# pretty-print per category metrics as Pandas DataFrame\n",
        "metric_table = dict()\n",
        "for id, label in id2label.items():\n",
        "    metric_table[label] = [\n",
        "                           metrics[\"per_category_iou\"][id],\n",
        "                           metrics[\"per_category_accuracy\"][id]\n",
        "    ]\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"per-category metrics:\")\n",
        "pd.DataFrame.from_dict(metric_table, orient=\"index\", columns=[\"IoU\", \"accuracy\"])"
      ],
      "metadata": {
        "id": "DezHZJUSqIL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_table"
      ],
      "metadata": {
        "id": "dDdnqK_nvQUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMpLvEROy4f"
      },
      "source": [
        "**Configuration**\n",
        "- Model: `VGG19`\n",
        "- Dataset: `CIFAR100`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU torch torchvision tdqm accelerate"
      ],
      "metadata": {
        "id": "BvSKeHiHO0ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46774fb3-0155-4604-cdcd-8ab35aec3f1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nVTwA6qVOy4j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nSr-O761Oy4k"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARN_RATE = 0.001\n",
        "NUM_EPOCHS = 5 # Much more than this isn't feasible w/o better hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQZbkQaBOy4k"
      },
      "outputs": [],
      "source": [
        "# TensorBoard setup\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = SummaryWriter(f\"runs/part_a2_{current_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54F8Oe0Oy4l"
      },
      "source": [
        "### Step 1. Load and Transform Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rc5RBEqAOy4l"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 224x224 for VGG19\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvsvwugeOy4l",
        "outputId": "9ee2c49e-286f-437d-f4d9-7a0f0d414777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 40702584.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYCqT2-Oy4m",
        "outputId": "33a14930-54bb-47c1-a5f1-3f0f1aaffb20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noA4xTn0Oy4n"
      },
      "source": [
        "### Step 2. Setup the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ro-8ZyOy4n",
        "outputId": "3e99587b-d7bc-4eec-85cd-0adfe2ad24aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 149MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "vgg19 = models.vgg19(pretrained=True)\n",
        "vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SyGSbKwFOy4n"
      },
      "outputs": [],
      "source": [
        "# Freeze the features part\n",
        "for param in vgg19.features.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "K2xjTQ1BOy4n"
      },
      "outputs": [],
      "source": [
        "# Modify the classifier part\n",
        "vgg19.classifier[6] = nn.Linear(4096, 100)  # CIFAR100 has 100 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65PObCi0Oy4n",
        "outputId": "ff4a6b6a-0158-4034-d09d-df8d80921fbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Make sure we're using GPU\n",
        "device = torch.device(\n",
        "    \"mps\"  # for macOS\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "vgg19 = vgg19.to(device)\n",
        "\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEuL9HNOy4o"
      },
      "source": [
        "### Step 3. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zerieDEEOy4o"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(vgg19.classifier.parameters(), lr=LEARN_RATE)  # Only train the classifier parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W83rW7_9Oy4o",
        "outputId": "1f3c2fec-0c2d-4d62-cf3f-75a12256b70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Batch 100 loss 4.044419877529144 accuracy 0.1046875\n",
            "Batch 200 loss 3.1314492440223693 accuracy 0.2421875\n",
            "Batch 300 loss 3.050539536476135 accuracy 0.2671875\n",
            "Batch 400 loss 3.0528750085830687 accuracy 0.2765625\n",
            "Batch 500 loss 3.0478632283210754 accuracy 0.2928125\n",
            "Batch 600 loss 3.094944188594818 accuracy 0.29375\n",
            "Batch 700 loss 3.0456761503219605 accuracy 0.28625\n",
            "Batch 800 loss 2.909969925880432 accuracy 0.3171875\n",
            "Batch 900 loss 2.8132975721359252 accuracy 0.334375\n",
            "Batch 1000 loss 2.806931126117706 accuracy 0.33875\n",
            "Batch 1100 loss 2.7887758576869963 accuracy 0.338125\n",
            "Batch 1200 loss 2.8247710263729093 accuracy 0.341875\n",
            "Batch 1300 loss 2.787786042690277 accuracy 0.34125\n",
            "Batch 1400 loss 2.800323547124863 accuracy 0.33625\n",
            "Batch 1500 loss 2.7855466341972352 accuracy 0.33375\n",
            "Epoch 2/5\n",
            "Batch 100 loss 2.5533981490135194 accuracy 0.3896875\n",
            "Batch 200 loss 2.5564373564720153 accuracy 0.3771875\n",
            "Batch 300 loss 2.5026396179199217 accuracy 0.3940625\n",
            "Batch 400 loss 2.6352894508838656 accuracy 0.3696875\n",
            "Batch 500 loss 2.4944422781467437 accuracy 0.4021875\n",
            "Batch 600 loss 2.6333390295505525 accuracy 0.376875\n",
            "Batch 700 loss 2.599839903116226 accuracy 0.39625\n",
            "Batch 800 loss 2.6051868426799776 accuracy 0.3803125\n",
            "Batch 900 loss 2.5946669936180116 accuracy 0.3875\n",
            "Batch 1000 loss 2.5592237877845765 accuracy 0.3834375\n",
            "Batch 1100 loss 2.6000115311145784 accuracy 0.393125\n",
            "Batch 1200 loss 2.609171509742737 accuracy 0.39\n",
            "Batch 1300 loss 2.605504915714264 accuracy 0.3775\n",
            "Batch 1400 loss 2.5224386298656465 accuracy 0.39875\n",
            "Batch 1500 loss 2.6126647877693174 accuracy 0.3796875\n",
            "Epoch 3/5\n",
            "Batch 100 loss 2.395217399597168 accuracy 0.423125\n",
            "Batch 200 loss 2.3372684693336487 accuracy 0.4309375\n",
            "Batch 300 loss 2.3548497104644777 accuracy 0.431875\n",
            "Batch 400 loss 2.361622359752655 accuracy 0.42875\n",
            "Batch 500 loss 2.3109984648227693 accuracy 0.4378125\n",
            "Batch 600 loss 2.3700219452381135 accuracy 0.426875\n",
            "Batch 700 loss 2.4531531631946564 accuracy 0.4265625\n",
            "Batch 800 loss 2.4170630288124086 accuracy 0.42625\n",
            "Batch 900 loss 2.36501628279686 accuracy 0.4371875\n",
            "Batch 1000 loss 2.418827431201935 accuracy 0.4334375\n",
            "Batch 1100 loss 2.408340882062912 accuracy 0.4228125\n",
            "Batch 1200 loss 2.356126638650894 accuracy 0.4315625\n",
            "Batch 1300 loss 2.4168195736408236 accuracy 0.425625\n",
            "Batch 1400 loss 2.4952066922187806 accuracy 0.41375\n",
            "Batch 1500 loss 2.452251431941986 accuracy 0.415\n",
            "Epoch 4/5\n",
            "Batch 100 loss 2.1940407657623293 accuracy 0.4715625\n",
            "Batch 200 loss 2.2594367825984953 accuracy 0.468125\n",
            "Batch 300 loss 2.3861572563648226 accuracy 0.4259375\n",
            "Batch 400 loss 2.3211678683757784 accuracy 0.4546875\n",
            "Batch 500 loss 2.3161070168018343 accuracy 0.4565625\n",
            "Batch 600 loss 2.325926887989044 accuracy 0.455625\n",
            "Batch 700 loss 2.270606005191803 accuracy 0.4615625\n",
            "Batch 800 loss 2.3532757329940797 accuracy 0.4359375\n",
            "Batch 900 loss 2.3076534986495973 accuracy 0.44625\n",
            "Batch 1000 loss 2.25383847117424 accuracy 0.4484375\n",
            "Batch 1100 loss 2.364472107887268 accuracy 0.4340625\n",
            "Batch 1200 loss 2.34172297000885 accuracy 0.4434375\n",
            "Batch 1300 loss 2.3479671657085417 accuracy 0.4409375\n",
            "Batch 1400 loss 2.3699635910987853 accuracy 0.435625\n",
            "Batch 1500 loss 2.299979475736618 accuracy 0.45\n",
            "Epoch 5/5\n",
            "Batch 100 loss 2.2186799168586733 accuracy 0.4796875\n",
            "Batch 200 loss 2.1654220855236055 accuracy 0.466875\n",
            "Batch 300 loss 2.3226944363117217 accuracy 0.451875\n",
            "Batch 400 loss 2.2329929268360136 accuracy 0.466875\n",
            "Batch 500 loss 2.291911416053772 accuracy 0.46125\n",
            "Batch 600 loss 2.1475585925579073 accuracy 0.4825\n",
            "Batch 700 loss 2.3928145641088485 accuracy 0.4621875\n",
            "Batch 800 loss 2.3835531842708586 accuracy 0.4403125\n",
            "Batch 900 loss 2.1857926094532014 accuracy 0.47375\n",
            "Batch 1000 loss 2.2549044954776765 accuracy 0.4625\n",
            "Batch 1100 loss 2.24351948261261 accuracy 0.4734375\n",
            "Batch 1200 loss 2.348175325393677 accuracy 0.45125\n",
            "Batch 1300 loss 2.2072887629270554 accuracy 0.4696875\n",
            "Batch 1400 loss 2.2931805515289305 accuracy 0.466875\n",
            "Batch 1500 loss 2.233938283920288 accuracy 0.4671875\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, criterion, optimizer, trainloader, num_epochs, device, writer):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if i % 100 == 99:\n",
        "                print(f'Batch {i + 1} loss {running_loss / 100} accuracy {correct / total}')\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "train_model(vgg19, criterion, optimizer, trainloader, NUM_EPOCHS, device, writer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, criterion, testloader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the network on the test images: {accuracy} %')\n",
        "    return test_loss, accuracy\n",
        "\n",
        "evaluate_model(vgg19, criterion, testloader, device)"
      ],
      "metadata": {
        "id": "SUzBN7YAIKnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5d668c-d22f-40cc-bb57-d9508ceeab39"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 49.81 %\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.9452560488789226, 49.81)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(vgg19.state_dict(), 'part_a2_vgg19_cifar100_model.pth')"
      ],
      "metadata": {
        "id": "knBMRCNEpWge"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}